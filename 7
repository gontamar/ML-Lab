#with sklearn
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import OneHotEncoder

file_path = r"weather_forecast.csv"
df = pd.read_csv(file_path)

print(df.head())

encoder = OneHotEncoder(drop='first')
X_encoded = encoder.fit_transform(df.drop('Play', axis=1)).toarray()
y = df['Play']

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

clf_id3 = DecisionTreeClassifier(criterion='entropy', random_state=42)

clf_id3.fit(X_train, y_train)

plt.figure(figsize=(12, 8))
plot_tree(clf_id3, filled=True, feature_names=encoder.get_feature_names_out(['Outlook', 'Temperature', 'Humidity', 'Windy']), class_names=['No', 'Yes'])
plt.show()

y_pred_id3 = clf_id3.predict(X_test)

accuracy_id3 = accuracy_score(y_test, y_pred_id3)
report_id3 = classification_report(y_test, y_pred_id3)

print("ID3 Algorithm Results:")
print(f"Accuracy: {accuracy_id3}")
print(f"Classification Report:\n{report_id3}")

cv_scores_id3 = cross_val_score(clf_id3, X_encoded, y, cv=5)
print("Cross-Validation Scores (ID3):", cv_scores_id3)
print("Mean CV Accuracy (ID3):", cv_scores_id3.mean())

clf_cart = DecisionTreeClassifier(criterion='gini', random_state=42)

clf_cart.fit(X_train, y_train)

plt.figure(figsize=(12, 8))
plot_tree(clf_cart, filled=True, feature_names=encoder.get_feature_names_out(['Outlook', 'Temperature', 'Humidity', 'Windy']), class_names=['No', 'Yes'])
plt.show()

y_pred_cart = clf_cart.predict(X_test)

accuracy_cart = accuracy_score(y_test, y_pred_cart)
report_cart = classification_report(y_test, y_pred_cart)

print("CART Algorithm Results:")
print(f"Accuracy: {accuracy_cart}")
print(f"Classification Report:\n{report_cart}")

cv_scores_cart = cross_val_score(clf_cart, X_encoded, y, cv=5)
print("Cross-Validation Scores (CART):", cv_scores_cart)
print("Mean CV Accuracy (CART):", cv_scores_cart.mean())


#without sklearn
import numpy as np
import pandas as pd
import math
from sklearn.model_selection import train_test_split

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value

def calculate_entropy(y):
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return -sum(p * math.log2(p) for p in probabilities if p > 0)

def calculate_gini(y):
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / len(y)
    return 1 - sum(p ** 2 for p in probabilities)

def calculate_gain(X, y, feature, threshold, criterion):
    left_indices = X[:, feature] < threshold
    y_left, y_right = y[left_indices], y[~left_indices]

    if criterion == "entropy":
        parent_metric = calculate_entropy(y)
        left_metric = calculate_entropy(y_left)
        right_metric = calculate_entropy(y_right)
    else:  # Gini impurity
        parent_metric = calculate_gini(y)
        left_metric = calculate_gini(y_left)
        right_metric = calculate_gini(y_right)
    
    weight_left = len(y_left) / len(y)
    weight_right = len(y_right) / len(y)
    
    gain = parent_metric - (weight_left * left_metric + weight_right * right_metric)
    return gain

def build_tree(X, y, max_depth, criterion):
    if len(set(y)) == 1 or max_depth == 0:
        return Node(value=np.bincount(y).argmax())
    
    n_features = X.shape[1]
    best_feature, best_threshold, best_gain = None, None, -1
    
    for feature in range(n_features):
        for threshold in np.unique(X[:, feature]):
            gain = calculate_gain(X, y, feature, threshold, criterion)
            if gain > best_gain:
                best_gain, best_feature, best_threshold = gain, feature, threshold
    
    if best_gain == 0:
        return Node(value=np.bincount(y).argmax())
    
    left_indices = X[:, best_feature] < best_threshold
    left_subtree = build_tree(X[left_indices], y[left_indices], max_depth - 1, criterion)
    right_subtree = build_tree(X[~left_indices], y[~left_indices], max_depth - 1, criterion)
    
    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)

def predict_tree(tree, x):
    if tree.value is not None:
        return tree.value
    if x[tree.feature] < tree.threshold:
        return predict_tree(tree.left, x)
    else:
        return predict_tree(tree.right, x)

def evaluate_tree(tree, X_test, y_test):
    y_pred = [predict_tree(tree, x) for x in X_test]
    accuracy = sum(y_pred == y_test) / len(y_test)
    return accuracy

df = pd.read_csv(r"C:\Users\gonta\Desktop\jssstu-ml-lab-main\7\weather_forecast.csv")

df = pd.get_dummies(df, drop_first=True)

X = df.drop('Play_Yes', axis=1).values
y = df['Play_Yes'].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

tree_id3 = build_tree(X_train, y_train, max_depth=5, criterion="entropy")
accuracy_id3 = evaluate_tree(tree_id3, X_test, y_test)
print("ID3 Algorithm Results:")
print(f"Accuracy: {accuracy_id3}")

tree_cart = build_tree(X_train, y_train, max_depth=5, criterion="gini")
accuracy_cart = evaluate_tree(tree_cart, X_test, y_test)
print("CART Algorithm Results:")
print(f"Accuracy: {accuracy_cart}")
